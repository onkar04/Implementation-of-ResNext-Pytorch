{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnext - checkpoint",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV7cKcz6UqI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Module - Resnext\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''Grouped convolution block.'''\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "        group_width = cardinality * bottleneck_width\n",
        "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(group_width)\n",
        "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(group_width)\n",
        "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*group_width:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*group_width)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
        "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
        "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
        "        # self.layer4 = self._make_layer(num_blocks[3], 2)\n",
        "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
        "\n",
        "    def _make_layer(self, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
        "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
        "        # Increase bottleneck_width by 2 after each stage.\n",
        "        self.bottleneck_width *= 2\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        # out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNeXt29_2x64d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n",
        "\n",
        "def ResNeXt29_4x64d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n",
        "\n",
        "def ResNeXt29_8x64d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n",
        "\n",
        "def ResNeXt29_32x4d():\n",
        "    return ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n",
        "\n",
        "def test_resnext():\n",
        "    net = ResNeXt29_2x64d()\n",
        "    x = torch.randn(1,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqyhF-yXUvbx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5194daf9-13f3-4bed-fb1e-53d7ae04f878"
      },
      "source": [
        "##main.py\n",
        "\n",
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "updatelr=input(\"want to update learning rate (default is 0.01) ? 1/0  \")\n",
        "\n",
        "if(updatelr==1):\n",
        "    lr=input(\"enter your learning rate, 0.1.0.01,0.001  \")\n",
        "else:\n",
        "    lr=0.01\n",
        "\n",
        "resume=input(\"resume? 1/0  \")\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "\n",
        "########################\n",
        "net = ResNeXt29_2x64d()\n",
        "net = net.to(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if (int(resume)==1):\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4).\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "      \n",
        "   \n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "     \n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            \n",
        "        \n",
        "            outputs = net(inputs)\n",
        "        \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "        \n",
        "        \n",
        " \n",
        "for epoch in range(start_epoch, start_epoch+50):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "==> Resuming from checkpoint..\n",
            "\n",
            "Epoch: 42\n",
            " [================================================================>]  Step: 4s933ms | Tot: 5m11s | Loss: 0.074 | Acc: 97.478% (48739/50000) 391/391 \n",
            " [================================================================>]  Step: 212ms | Tot: 21s337ms | Loss: 0.557 | Acc: 88.410% (8841/10000) 100/100 \n",
            "\n",
            "Epoch: 43\n",
            " [================================================================>]  Step: 500ms | Tot: 5m7s | Loss: 0.071 | Acc: 97.532% (48766/50000) 391/391 \n",
            " [================================================================>]  Step: 212ms | Tot: 21s268ms | Loss: 0.567 | Acc: 87.960% (8796/10000) 100/100 \n",
            "\n",
            "Epoch: 44\n",
            " [================================================================>]  Step: 504ms | Tot: 5m6s | Loss: 0.064 | Acc: 97.706% (48853/50000) 391/391 \n",
            " [================================================================>]  Step: 215ms | Tot: 21s267ms | Loss: 0.595 | Acc: 88.130% (8813/10000) 100/100 \n",
            "\n",
            "Epoch: 45\n",
            " [================================================================>]  Step: 498ms | Tot: 5m7s | Loss: 0.065 | Acc: 97.640% (48820/50000) 391/391 \n",
            " [================================================================>]  Step: 213ms | Tot: 21s336ms | Loss: 0.535 | Acc: 88.890% (8889/10000) 100/100 \n",
            "\n",
            "Epoch: 46\n",
            " [================================================================>]  Step: 499ms | Tot: 5m6s | Loss: 0.063 | Acc: 97.850% (48925/50000) 391/391 \n",
            " [================================================================>]  Step: 215ms | Tot: 21s216ms | Loss: 0.668 | Acc: 86.960% (8696/10000) 100/100 \n",
            "\n",
            "Epoch: 47\n",
            " [================================================================>]  Step: 500ms | Tot: 5m6s | Loss: 0.061 | Acc: 97.864% (48932/50000) 391/391 \n",
            " [================================================================>]  Step: 216ms | Tot: 21s245ms | Loss: 0.522 | Acc: 88.870% (8887/10000) 100/100 \n",
            "\n",
            "Epoch: 48\n",
            " [================================================================>]  Step: 496ms | Tot: 5m6s | Loss: 0.057 | Acc: 98.000% (49000/50000) 391/391 \n",
            " [================================================================>]  Step: 210ms | Tot: 21s400ms | Loss: 0.631 | Acc: 87.610% (8761/10000) 100/100 \n",
            "\n",
            "Epoch: 49\n",
            " [================================================================>]  Step: 500ms | Tot: 5m6s | Loss: 0.059 | Acc: 97.908% (48954/50000) 391/391 \n",
            " [================================================================>]  Step: 213ms | Tot: 21s197ms | Loss: 0.498 | Acc: 89.660% (8966/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 50\n",
            " [================================================================>]  Step: 498ms | Tot: 5m6s | Loss: 0.052 | Acc: 98.170% (49085/50000) 391/391 \n",
            " [================================================================>]  Step: 218ms | Tot: 21s294ms | Loss: 0.628 | Acc: 88.060% (8806/10000) 100/100 \n",
            "\n",
            "Epoch: 51\n",
            " [================================================================>]  Step: 497ms | Tot: 5m6s | Loss: 0.061 | Acc: 97.954% (48977/50000) 391/391 \n",
            " [================================================================>]  Step: 214ms | Tot: 21s342ms | Loss: 0.571 | Acc: 88.600% (8860/10000) 100/100 \n",
            "\n",
            "Epoch: 52\n",
            " [================================================================>]  Step: 494ms | Tot: 5m6s | Loss: 0.051 | Acc: 98.194% (49097/50000) 391/391 \n",
            " [================================================================>]  Step: 216ms | Tot: 21s269ms | Loss: 0.594 | Acc: 88.800% (8880/10000) 100/100 \n",
            "\n",
            "Epoch: 53\n",
            " [================================================================>]  Step: 504ms | Tot: 5m6s | Loss: 0.055 | Acc: 98.112% (49056/50000) 391/391 \n",
            " [================================================================>]  Step: 214ms | Tot: 21s293ms | Loss: 0.544 | Acc: 89.120% (8912/10000) 100/100 \n",
            "\n",
            "Epoch: 54\n",
            " [================================================================>]  Step: 502ms | Tot: 5m6s | Loss: 0.051 | Acc: 98.222% (49111/50000) 391/391 \n",
            " [================================================================>]  Step: 216ms | Tot: 21s313ms | Loss: 0.696 | Acc: 87.760% (8776/10000) 100/100 \n",
            "\n",
            "Epoch: 55\n",
            " [================================================================>]  Step: 504ms | Tot: 5m6s | Loss: 0.046 | Acc: 98.414% (49207/50000) 391/391 \n",
            " [================================================================>]  Step: 218ms | Tot: 21s246ms | Loss: 0.555 | Acc: 89.280% (8928/10000) 100/100 \n",
            "\n",
            "Epoch: 56\n",
            " [================================================================>]  Step: 494ms | Tot: 5m6s | Loss: 0.048 | Acc: 98.422% (49211/50000) 391/391 \n",
            " [================================================================>]  Step: 209ms | Tot: 21s229ms | Loss: 0.631 | Acc: 88.560% (8856/10000) 100/100 \n",
            "\n",
            "Epoch: 57\n",
            " [================================================================>]  Step: 496ms | Tot: 5m6s | Loss: 0.047 | Acc: 98.386% (49193/50000) 391/391 \n",
            " [================================================================>]  Step: 211ms | Tot: 21s217ms | Loss: 0.623 | Acc: 88.810% (8881/10000) 100/100 \n",
            "\n",
            "Epoch: 58\n",
            " [================================================================>]  Step: 497ms | Tot: 5m6s | Loss: 0.046 | Acc: 98.374% (49187/50000) 391/391 \n",
            " [================================================================>]  Step: 216ms | Tot: 21s240ms | Loss: 0.718 | Acc: 87.230% (8723/10000) 100/100 \n",
            "\n",
            "Epoch: 59\n",
            " [================================================================>]  Step: 502ms | Tot: 5m6s | Loss: 0.049 | Acc: 98.320% (49160/50000) 391/391 \n",
            " [================================================================>]  Step: 210ms | Tot: 21s272ms | Loss: 0.590 | Acc: 89.380% (8938/10000) 100/100 \n",
            "\n",
            "Epoch: 60\n",
            " [================================================================>]  Step: 504ms | Tot: 5m6s | Loss: 0.045 | Acc: 98.378% (49189/50000) 391/391 \n",
            " [================================================================>]  Step: 215ms | Tot: 21s226ms | Loss: 0.610 | Acc: 89.040% (8904/10000) 100/100 \n",
            "\n",
            "Epoch: 61\n",
            " [================================================================>]  Step: 501ms | Tot: 5m6s | Loss: 0.041 | Acc: 98.628% (49314/50000) 391/391 \n",
            " [================================================================>]  Step: 209ms | Tot: 21s198ms | Loss: 0.720 | Acc: 87.370% (8737/10000) 100/100 \n",
            "\n",
            "Epoch: 62\n",
            " [================================================================>]  Step: 494ms | Tot: 5m5s | Loss: 0.050 | Acc: 98.336% (49168/50000) 391/391 \n",
            " [================================================================>]  Step: 206ms | Tot: 21s221ms | Loss: 0.570 | Acc: 89.230% (8923/10000) 100/100 \n",
            "\n",
            "Epoch: 63\n",
            " [================================================================>]  Step: 495ms | Tot: 5m5s | Loss: 0.044 | Acc: 98.468% (49234/50000) 391/391 \n",
            " [================================================================>]  Step: 210ms | Tot: 21s271ms | Loss: 0.601 | Acc: 89.420% (8942/10000) 100/100 \n",
            "\n",
            "Epoch: 64\n",
            " [================================================================>]  Step: 501ms | Tot: 5m5s | Loss: 0.043 | Acc: 98.536% (49268/50000) 391/391 \n",
            " [================================================================>]  Step: 214ms | Tot: 21s241ms | Loss: 0.647 | Acc: 89.010% (8901/10000) 100/100 \n",
            "\n",
            "Epoch: 65\n",
            " [================================================================>]  Step: 501ms | Tot: 5m5s | Loss: 0.041 | Acc: 98.642% (49321/50000) 391/391 \n",
            " [================================================================>]  Step: 216ms | Tot: 21s291ms | Loss: 0.701 | Acc: 87.920% (8792/10000) 100/100 \n",
            "\n",
            "Epoch: 66\n",
            " [================================================================>]  Step: 502ms | Tot: 5m5s | Loss: 0.046 | Acc: 98.436% (49218/50000) 391/391 \n",
            " [================================================================>]  Step: 219ms | Tot: 21s280ms | Loss: 0.631 | Acc: 89.660% (8966/10000) 100/100 \n",
            "\n",
            "Epoch: 67\n",
            " [================================================================>]  Step: 501ms | Tot: 5m5s | Loss: 0.038 | Acc: 98.702% (49351/50000) 391/391 \n",
            " [================================================================>]  Step: 212ms | Tot: 21s282ms | Loss: 0.621 | Acc: 88.920% (8892/10000) 100/100 \n",
            "\n",
            "Epoch: 68\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8766cc78f3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8766cc78f3ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5dmqiaQUxqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## utils.py\n",
        "\n",
        "\n",
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "#_, term_width = os.popen('stty size', 'r').read().split()\n",
        "\n",
        "\n",
        "_, term_width = shutil.get_terminal_size()\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "\n",
        "\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3oTVIrsU17q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b01c527d-8c60-4285-8211-23be4e976e2f"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7f7adc2a8ef0>\n",
            "1\n",
            "Tesla K80\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq6kuMHRU2kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "\n",
        "os.mkdir('checkpoint')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSk0pmvVVBfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4c0506a-fa71-438b-ff1a-fe6e468976f8"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.move(\"/content/ckpt (1).pth\",\"/content/checkpoint\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/checkpoint/ckpt (1).pth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}